{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70372cf-ff7b-44c8-b2e1-7b4d9c0e9016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d105c-79d7-4090-bf8a-1bd33c3bb17e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f6dbed-abe3-46db-b1f1-e7766693428a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba7ef90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from flask import request, Flask, jsonify\n",
    "from flasgger import Swagger, LazyString, LazyJSONEncoder, swag_from\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sqlite3\n",
    "app = Flask(__name__)\n",
    "\n",
    "app.json_encoder = LazyJSONEncoder\n",
    "swagger_template = dict(\n",
    "info = {\n",
    "    'title': LazyString(lambda: 'API Documentation for Data Processing and Modeling'),\n",
    "    'version': LazyString(lambda: '1.0.0'),\n",
    "    'description': LazyString(lambda: 'Dokumentasi API untuk Data Processing and Modeling')\n",
    "    },\n",
    "    host = LazyString(lambda: request.host)\n",
    ")\n",
    "\n",
    "swagger_config = {\n",
    "    \"headers\": [],\n",
    "    \"specs\": [\n",
    "        {\n",
    "            \"endpoint\": 'docs',\n",
    "            \"route\": '/docs.json'\n",
    "        }\n",
    "    ],\n",
    "    \"static_url_path\": \"/flasgger_static\",\n",
    "    \"swagger_ui\": True,\n",
    "    \"specs_route\": \"/docs/\"\n",
    "}\n",
    "swagger = Swagger(app, template=swagger_template,\n",
    "                 config=swagger_config)\n",
    "\n",
    "df_abusive = pd.read_csv('abusive.csv', encoding='latin-1')\n",
    "\n",
    "@swag_from(\"C:/Users/User/Documents/data science/API/docs/hello_world.yml\", methods=['GET'])\n",
    "@app.route('/', methods=['GET'])\n",
    "def hello_world():\n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Menyapa Hello World\",\n",
    "        'data': \"Hello World\"\n",
    "    }\n",
    "    \n",
    "    response_data=jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "@swag_from(\"C:/Users/User/Documents/data science/docs/text_processing.yml\", methods=['POST'])\n",
    "@app.route('/text-processing', methods=['POST'])\n",
    "\n",
    "def text_processing():\n",
    "    \n",
    "    text = request.form.get('text')\n",
    "    \n",
    "    \n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Teks yang sudah diproses\",\n",
    "        'data': re.sub(r'[^a-zA-Z0-9]',' ',text),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    response_data=jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "@swag_from(\"C:/Users/User/Documents/data science/docs/file_processing.yml\", methods=['POST'])\n",
    "@app.route('/text-processing-file', methods=['POST'])\n",
    "def text_processing_file():\n",
    "    global file, df, new_df, final_df, sql_database\n",
    "    \n",
    "    file = request.files.get('file')\n",
    "    df = pd.read_csv(file, encoding='latin-1', nrows=20)\n",
    "    \n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"File berisi teks yang akan diproses\",\n",
    "        'data': \"File berhasil diinput dan diproses\"\n",
    "    }\n",
    "    # menghilangkan data yang duplikat\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # cleansing data menggunakan regex\n",
    "    new_list = []\n",
    "    for i in range(len(df)):\n",
    "        cleaned_text_a = re.sub(r'[^a-zA-Z0-9]',\" \" ,df['Tweet'].iloc[i])\n",
    "        cleaned_text_b = re.sub('RT',' ', cleaned_text_a)\n",
    "        cleaned_text_c = re.sub('\\\\+n',' ', cleaned_text_b)\n",
    "        cleaned_text_d = re.sub('rt',' ', cleaned_text_c)\n",
    "        cleaned_text_e = re.sub('user',' ', cleaned_text_d)\n",
    "        cleaned_text_f = re.sub('USER',' ', cleaned_text_e)\n",
    "        cleaned_text_g = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+)|(URL))','', cleaned_text_f)\n",
    "        cleaned_text_h = re.sub(':',' ', cleaned_text_g)\n",
    "        cleaned_text_i = re.sub(';',' ', cleaned_text_h)\n",
    "        cleaned_text_j = re.sub('\\\\+',' ', cleaned_text_i)\n",
    "        cleaned_text_k = re.sub('  +',' ', cleaned_text_j)\n",
    "        cleaned_text_l = re.sub('x..',' ', cleaned_text_k)\n",
    "        cleaned_text_m = re.sub(' n ',' ', cleaned_text_l)\n",
    "        new_list.append(cleaned_text_m)\n",
    "       \n",
    "    # masukkan proses cleansing data pada kolom Tweet\n",
    "    new_df = df.copy()\n",
    "    new_df['Tweet'] = new_list\n",
    "    \n",
    "    # membuat dataframe dengan menggabungkan kolom Tweet yang sudah dibersihkan dan sebelum dibersihkan dan menambahkan kolom number character dan number word \n",
    "    final_df = pd.concat([df['Tweet'],new_df['Tweet']],axis=1)\n",
    "    final_df.columns = ['old','new']\n",
    "    final_df['no_char_old'] = final_df['old'].apply(len)\n",
    "    final_df['no_word_old'] = final_df['old'].apply(lambda x: len(x.split()))\n",
    "    final_df['no_char_new'] = final_df['new'].apply(len)\n",
    "    final_df['no_word_new'] = final_df['new'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    # buat fungsi untuk mengetahui berapa abusive word pada setiap baris\n",
    "    def count_abusive(x):\n",
    "        cleaned_tweet = x\n",
    "        matched_list = []\n",
    "        for i in range(len(df_abusive)):\n",
    "            for j in x.split():\n",
    "                word = df_abusive['ABUSIVE'].iloc[i]\n",
    "                if word==j.lower():\n",
    "                    matched_list.append(word)\n",
    "        return len(matched_list)\n",
    "    \n",
    "    # masukkan fungsi tersebut kedalam kolom baru yang berisi jumlah abusive word pada tweet\n",
    "    final_df['no_abusive'] = final_df['new'].apply(lambda x: count_abusive(x))\n",
    "    \n",
    "    # buat database menggunakan sqlite3\n",
    "    conn = sqlite3.connect('test1.db')\n",
    "    q_create_table = \"\"\"\n",
    "    create table final_df (old varchar(255), new varchar(255), no_char_old int, no_word_old int, no_char_new int, no_word_new int, no_abusive int);\n",
    "    \"\"\"\n",
    "    conn.execute(q_create_table)\n",
    "    conn.commit()\n",
    "    \n",
    "    # DO ITERATIONS TO INSERT DATA (EACH ROW) FROM FINAL DATAFRAME (POST_DF)\n",
    "    for i in range(len(final_df)):\n",
    "        old = final_df['old'].iloc[i]\n",
    "        new = final_df['new'].iloc[i]\n",
    "        no_char_old = int(final_df['no_char_old'].iloc[i])\n",
    "        no_word_old = int(final_df['no_word_old'].iloc[i])\n",
    "        no_char_new = int(final_df['no_char_new'].iloc[i])\n",
    "        no_word_new = int(final_df['no_word_new'].iloc[i])\n",
    "        no_abusive = int(final_df['no_abusive'].iloc[i])\n",
    "    \n",
    "        q_insertion = \"insert into final_df (old, new, no_char_old, no_word_old, no_char_new, no_word_new, no_abusive) values (?,?,?,?,?,?,?)\"\n",
    "        conn.execute(q_insertion,(old,new,no_char_old,no_word_old,no_char_new,no_word_new,no_abusive))\n",
    "        conn.commit()\n",
    "        \n",
    "    conn.close()\n",
    "    \n",
    "    plt.figure(figsize=(10,7))\n",
    "    countplot = sns.countplot(data=final_df, x=\"no_abusive\")\n",
    "    for p in countplot.patches:\n",
    "        countplot.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() / 2., p.get_height()),  ha = 'center'\n",
    "                            , va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
    "\n",
    "    # %matplotlib inline\n",
    "    # warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "    plt.title('Count of Estimated Number of Abusive Words')\n",
    "    plt.xlabel('Estimated Number of Abusive Words')\n",
    "    plt.savefig('new_countplot.jpeg')\n",
    "    \n",
    "    plt.figure(figsize=(20,4))\n",
    "    boxplot = sns.boxplot(data=final_df, x=\"no_word_new\")\n",
    "\n",
    "    print()\n",
    "    \n",
    "    # VISUALIZE THE NUMBER OF WORDS USING BOXPLOT\n",
    "    # %matplotlib inline\n",
    "    # warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "    plt.title('Number of Words Boxplot (after tweet cleansing)')\n",
    "    plt.xlabel('')\n",
    "    plt.savefig('new_boxplot.jpeg')\n",
    "    \n",
    "    \n",
    "    \n",
    "   # OUTPUT THE RESULT IN JSON FORMAT\n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Teks yang sudah diproses\",\n",
    "        'data': list(final_df['new'])\n",
    "    }\n",
    "    \n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9e14cb-5343-4193-be8b-a0a06579f56e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0d59df-3c16-47ff-97ce-946986addaed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a33c1-007f-4dce-af4b-d4b329b93ad7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_abusive(x):\n",
    "        cleaned_tweet = x\n",
    "        matched_list = []\n",
    "        for i in range(len(df_abusive)):\n",
    "            for j in x.split():\n",
    "                word = df_abusive['ABUSIVE'].iloc[i]\n",
    "                if word==j.lower():\n",
    "                    matched_list.append(word)\n",
    "        return len(matched_list)\n",
    "    final_df['no_abusive'] = final_df['new'].apply(lambda x: count_abusive(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243f7886-2837-4fce-b7c2-f2f747ff5d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
